import os
import subprocess
import sys

# Step 1: Ensure Required Modules are Installed
def install_package(package):
    """Function to install a Python package via pip."""
    try:
        __import__(package)
    except ImportError:
        print(f"Installing {package}...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", package])

# Install required packages
install_package("pyspark")

# Step 2: Import Installed Modules
from pyspark.sql import SparkSession
from pyspark.sql.functions import when, col

# Step 3: Initialize Spark Session
spark = SparkSession.builder.appName("MusicDataAnalysis").getOrCreate()

# Step 4: Define File Paths
# Replace this with the actual path to your CSV file
file_path = "/home/cloudera/music_tracks.csv"
output_path = "/home/cloudera/high_energy_tracks.csv"

# Step 5: Data Loading and Schema Understanding
print("Loading the dataset...")
music_df = spark.read.csv(file_path, header=True, inferSchema=True)

print("Schema of the dataset:")
music_df.printSchema()

print("Sample data:")
music_df.show(5)

# Step 6: Data Aggregation
print("Calculating average metrics by artist...")
agg_df = music_df.groupBy("artist_name").agg(
    {"danceability": "avg", "energy": "avg", "tempo": "avg"}
).withColumnRenamed("avg(danceability)", "avg_danceability") \
 .withColumnRenamed("avg(energy)", "avg_energy") \
 .withColumnRenamed("avg(tempo)", "avg_tempo")

print("Average metrics by artist:")
agg_df.show()

print("Finding the top 5 artists by track popularity...")
top_artists = music_df.groupBy("artist_name").agg({"popularity": "avg"}) \
    .withColumnRenamed("avg(popularity)", "avg_popularity") \
    .orderBy("avg_popularity", ascending=False).limit(5)

print("Top 5 artists by popularity:")
top_artists.show()

# Step 7: Data Transformation
print("Classifying tracks based on energy level...")
transformed_df = music_df.withColumn(
    "energy_level",
    when(col("energy") > 0.8, "High Energy").otherwise("Regular Energy")
)

print("Data with energy_level classification:")
transformed_df.show()

print("Aggregating average metrics by energy level...")
energy_agg_df = transformed_df.groupBy("energy_level").agg(
    {"popularity": "avg", "loudness": "avg"}
).withColumnRenamed("avg(popularity)", "avg_popularity") \
 .withColumnRenamed("avg(loudness)", "avg_loudness")

print("Average metrics by energy level:")
energy_agg_df.show()

# Step 8: Data Exporting
print("Exporting 'High Energy' tracks...")
high_energy_df = transformed_df.filter(col("energy_level") == "High Energy")
high_energy_df.write.csv(output_path, header=True)

print(f"High energy data exported to: {output_path}")

# Step 9: Stop Spark Session
spark.stop()
print("Spark session stopped.")
